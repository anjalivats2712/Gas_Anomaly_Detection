{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from pyentrp import entropy as ent\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = '/home/project/gas_anormaly_detection/restaurant/1training/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = path.join(project_root, 'Dataset')\n",
    "save_data_root = path.join(project_root, '2pattern_extraction/save_data')\n",
    "if not path.exists(save_data_root):\n",
    "    os.mkdir(save_data_root)\n",
    "    print(\"make a new dir.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = np.load(path.join(save_data_root, 'norm_sample_intra.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max normalization\n",
    "train_samples_normlization = []\n",
    "for i in range(train_samples.shape[0]):\n",
    "    train_sample_temp = train_samples[i]\n",
    "    max_value = max(train_sample_temp)\n",
    "    train_sample_temp = train_sample_temp/max_value\n",
    "    train_samples_normlization.append(train_sample_temp)\n",
    "train_samples_normlization = np.array(train_samples_normlization)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transfor to day entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_train_samples = []\n",
    "for i in range(train_samples_normlization.shape[0]):\n",
    "    sample_data = train_samples_normlization[i]\n",
    "    entropy_feature = []\n",
    "    for j in range(7):\n",
    "        day_data = sample_data[j*24:j*24+24]\n",
    "        entropy = ent.permutation_entropy(day_data,order=3,delay=1,normalize=True)\n",
    "        entropy_feature.append(entropy)\n",
    "    entropy_feature = np.array(entropy_feature)\n",
    "    entropy_train_samples.append(entropy_feature)\n",
    "\n",
    "entropy_train_samples = np.array(entropy_train_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = []\n",
    "for i in range(5,20):\n",
    "    km = KMeans(n_clusters=i,random_state=0)\n",
    "    km.fit(entropy_train_samples)\n",
    "\n",
    "    labels = km.labels_\n",
    "    sc.append(sklearn.metrics.silhouette_score(entropy_train_samples, labels,metric='euclidean'))\n",
    "    \n",
    "plt.rcParams['figure.figsize'] = (24.0, 8.0)\n",
    "plt.plot(range(5,20),sc,marker=\"o\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"sc\")\n",
    "plt.xlim([4,20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_num = 10\n",
    "estimator_ed_dtw = KMeans(n_clusters=clusters_num,random_state=0)\n",
    "estimator_ed_dtw.fit(entropy_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred = estimator_ed_dtw.labels_ \n",
    "cluster_centers = estimator_ed_dtw.cluster_centers_\n",
    "cluster_sample_index = {}\n",
    "intra_cluster_distances ={}\n",
    "cluster_distance_mean ={}\n",
    "cluster_sample_num ={}\n",
    "\n",
    "for i in range(len(label_pred)):\n",
    "    label=label_pred[i]\n",
    "    sample = entropy_train_samples[i]\n",
    "    center = cluster_centers[label]\n",
    "    distance_temp = np.linalg.norm(sample-center)\n",
    "\n",
    "    if label in cluster_sample_num.keys():\n",
    "        sample_num = cluster_sample_num[label]\n",
    "        distance_list = intra_cluster_distances[label]\n",
    "        distance_list.append(distance_temp)\n",
    "        distance_mean = cluster_distance_mean[label]\n",
    "        distance_mean = distance_mean+distance_temp\n",
    "        sample_num+=1\n",
    "        \n",
    "        sample_indexs = cluster_sample_index[label]\n",
    "        sample_indexs.append(i)\n",
    "        \n",
    "        cluster_sample_num[label]=sample_num\n",
    "        intra_cluster_distances[label]=distance_list\n",
    "        cluster_sample_index[label]=sample_indexs\n",
    "        cluster_distance_mean[label]=distance_mean\n",
    "        \n",
    "    else:\n",
    "        cluster_sample_num[label]=1\n",
    "        intra_cluster_distances[label]=[distance_temp]\n",
    "        cluster_sample_index[label]=[i]\n",
    "        cluster_distance_mean[label]=distance_temp\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in cluster_distance_mean.keys():\n",
    "    distance = cluster_distance_mean[key]\n",
    "    sample_num = cluster_sample_num[key]\n",
    "    distance = distance/sample_num\n",
    "    cluster_distance_mean[key]=distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(cluster_distance_mean.items(), key=lambda item:item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cluster-level consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_distance_mean_np = []\n",
    "for i in range(clusters_num):\n",
    "    cluster_distance_mean_np.append(cluster_distance_mean[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_clusters = []\n",
    "for i in range(clusters_num):\n",
    "    class_mean = cluster_distance_mean_np[i]\n",
    "    all_mean = []\n",
    "    \n",
    "    for j in range(clusters_num):\n",
    "        if i!=j: \n",
    "            all_mean.append(cluster_distance_mean_np[j]) \n",
    "    if class_mean>np.mean(all_mean)+2*np.std(all_mean):\n",
    "        bad_clusters.append(i)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_index =[]\n",
    "for label in bad_clusters:\n",
    "    anomaly_index.extend(cluster_sample_index[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instance-level consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in range(clusters_num):\n",
    "    if label not in bad_clusters:\n",
    "        sample_index = np.array(cluster_sample_index[label])\n",
    "        cluster_distances = np.array(intra_cluster_distances[label])\n",
    "        anomaly_index_temp = sample_index[np.where(cluster_distances>np.mean(cluster_distances)+2*np.std(cluster_distances))[0]]\n",
    "        anomaly_index.extend(anomaly_index_temp)\n",
    "print(len(anomaly_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnorm_sample_np = train_samples[anomaly_index]\n",
    "\n",
    "nomal_index = [val for val in range(len(train_samples)) if val not in anomaly_index]\n",
    "train_sample_np = train_samples[nomal_index]\n",
    "\n",
    "print(abnorm_sample_np.shape)\n",
    "print(train_sample_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train test set\n",
    "np.save(path.join(save_data_root, 'abnorm_sample_inter.npy'), abnorm_sample_np)\n",
    "np.save(path.join(save_data_root, 'train_sample_inter.npy'), train_sample_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
