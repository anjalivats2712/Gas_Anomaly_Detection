{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import path\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_root\n",
    "project_root = '/home/project/gas_anormaly_detection/restaurant/1training/'\n",
    "data_root = path.join(project_root, 'Dataset/')\n",
    "raw_restaurant_root = path.join(project_root, 'Dataset/restaurant/')\n",
    "raw_restaurant_list = os.listdir(raw_restaurant_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. segement data into weekly sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_cols = []\n",
    "daysample_points =24\n",
    "week_cols.extend(['pre'])\n",
    "week_cols.extend(['Mon'] * daysample_points)\n",
    "week_cols.extend(['Tues'] * daysample_points)\n",
    "week_cols.extend(['Wed'] * daysample_points)\n",
    "week_cols.extend(['Thur'] * daysample_points)\n",
    "week_cols.extend(['Fri'] * daysample_points)\n",
    "week_cols.extend(['Sat'] * daysample_points)\n",
    "week_cols.extend(['Sun'] * daysample_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weekly data and weekly date\n",
    "meter_week_data_path = path.join(data_root, 'meter_week_data')\n",
    "meter_week_date_path = path.join(data_root, 'meter_week_date')\n",
    "if not os.path.exists(meter_week_data_path):\n",
    "    os.mkdir(meter_week_data_path)\n",
    "    print(\"creat new data dir.\")\n",
    "\n",
    "if not os.path.exists(meter_week_date_path):\n",
    "    os.mkdir(meter_week_date_path)\n",
    "    print(\"creat new date dir.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the first monday\n",
    "def find_monday(date_data):\n",
    "    i = 0\n",
    "    while 1:\n",
    "        date_string = date_data[i]\n",
    "        date = datetime.strptime(date_string, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        if date.weekday() == 0 and date_string.split(' ')[1]=='00:00:00':\n",
    "            break\n",
    "        else:\n",
    "            i = i+1\n",
    "    return date_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 0\n",
    "\n",
    "for f_name in tqdm(raw_restaurant_list):\n",
    "\n",
    "    meter_week_data = pd.DataFrame(columns=week_cols)\n",
    "    meter_week_date = pd.DataFrame(columns=week_cols)\n",
    "\n",
    "    tmp_data = pd.read_csv(os.path.join(raw_restaurant_root, f_name))   \n",
    "\n",
    "    id = os.path.splitext(f_name)[0]\n",
    "\n",
    "    first_day = find_monday(tmp_data['Date'])\n",
    "\n",
    "    first_index = tmp_data[tmp_data['Date'] == first_day].index.values[0]\n",
    "    last_index = first_index + daysample_points * 7\n",
    "\n",
    "    while (last_index < tmp_data.shape[0]):\n",
    "\n",
    "        # save weekly data\n",
    "        if first_index>0:\n",
    "            append_data = tmp_data.iloc[first_index-1:last_index]['AccumFlow'].values\n",
    "        else:\n",
    "            append_data = np.append(np.array([np.nan]), tmp_data.iloc[first_index:last_index]['AccumFlow'].values)\n",
    "        \n",
    "        tmp_data_df = pd.DataFrame([append_data], columns=week_cols)\n",
    "        meter_week_data = pd.concat([meter_week_data, tmp_data_df], ignore_index=True)\n",
    "\n",
    "        # save weekly date\n",
    "        if first_index>0:\n",
    "            append_date = tmp_data.iloc[first_index-1:last_index]['Date'].values\n",
    "        else:\n",
    "            append_date = np.append(np.array([np.nan]), tmp_data.iloc[first_index:last_index]['Date'].values)\n",
    "        \n",
    "        tmp_date_df = pd.DataFrame([append_date], columns=week_cols)\n",
    "        meter_week_date = pd.concat([meter_week_date, tmp_date_df], ignore_index=True)\n",
    "\n",
    "        first_index = last_index\n",
    "        last_index = first_index + daysample_points * 7\n",
    "        sample_num+=1\n",
    "\n",
    "    meter_week_data.to_csv(path.join(meter_week_data_path, f_name), index=False)\n",
    "    meter_week_date.to_csv(path.join(meter_week_date_path, f_name), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  calculate gas consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter_week_data_list = os.listdir(meter_week_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_week_data_path = path.join(data_root, 'hourly_week_data')\n",
    "hourly_week_date_path = path.join(data_root, 'hourly_week_date')\n",
    "if not os.path.exists(hourly_week_data_path):\n",
    "    os.mkdir(hourly_week_data_path)\n",
    "    print(\"creat new data dir.\")\n",
    "\n",
    "if not os.path.exists(hourly_week_date_path):\n",
    "    os.mkdir(hourly_week_date_path)\n",
    "    print(\"creat new date dir.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len=daysample_points*7\n",
    "\n",
    "nan_delet_num =0\n",
    "nan_delet_samples = []\n",
    "\n",
    "sample_num_after_delet = 0 \n",
    "\n",
    "for f_name in tqdm(meter_week_data_list):\n",
    "    hourly_week_data = pd.DataFrame(columns=week_cols[1:])\n",
    "    hourly_week_date = pd.DataFrame(columns=week_cols[1:])\n",
    "\n",
    "    meter_week_data = pd.read_csv(os.path.join(meter_week_data_path, f_name))  \n",
    "    meter_week_date = pd.read_csv(os.path.join(meter_week_date_path, f_name)) \n",
    "    for idx in range(meter_week_data.shape[0]):\n",
    "        week_data= meter_week_data.iloc[idx,:].values\n",
    "        week_date= meter_week_date.iloc[idx,:].values[1:]\n",
    "\n",
    "        week_data[week_data<=0] = np.nan\n",
    "        week_data = np.diff(week_data)\n",
    "        \n",
    "        if np.isnan(week_data).sum()>0:\n",
    "            nan_delet_num+=1\n",
    "            nan_delet_samples.append(week_data)\n",
    "            continue\n",
    "\n",
    "        sample_num_after_delet+=1\n",
    "        week_data = pd.DataFrame([week_data], columns=week_cols[1:])\n",
    "        week_date = pd.DataFrame([week_date], columns=week_cols[1:])\n",
    "\n",
    "        hourly_week_data = pd.concat([hourly_week_data, week_data], ignore_index=True)\n",
    "        hourly_week_date = pd.concat([hourly_week_date, week_date], ignore_index=True)\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "    hourly_week_data.to_csv(path.join(hourly_week_data_path, f_name), index=False)\n",
    "    hourly_week_date.to_csv(path.join(hourly_week_date_path, f_name), index=False)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. rule-based anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_week_data_list = os.listdir(hourly_week_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_week_data_afterrules_path = path.join(data_root, 'hourly_week_data_afterrules')\n",
    "hourly_week_date_afterrules_path = path.join(data_root, 'hourly_week_date_afterrules')\n",
    "if not os.path.exists(hourly_week_data_afterrules_path):\n",
    "    os.mkdir(hourly_week_data_afterrules_path)\n",
    "    print(\"creat new data dir.\")\n",
    "\n",
    "if not os.path.exists(hourly_week_date_afterrules_path):\n",
    "    os.mkdir(hourly_week_date_afterrules_path)\n",
    "    print(\"creat new date dir.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_consumption_anomalies = []\n",
    "negative_consumption_anomalies = []\n",
    "burty_consumption_anomalies = []\n",
    "sample_num_after_rules = 0\n",
    "\n",
    "for f_name in tqdm(hourly_week_data_list):\n",
    "    id_ = os.path.splitext(f_name)[0]\n",
    "    hourly_week_data_preprocessed = pd.DataFrame(columns=week_cols)\n",
    "    hourly_week_date_preprocessed = pd.DataFrame(columns=week_cols)\n",
    "\n",
    "    hourly_week_data = pd.read_csv(os.path.join(hourly_week_data_path, f_name))  \n",
    "    hourly_week_date = pd.read_csv(os.path.join(hourly_week_date_path, f_name)) \n",
    "    week_num = 0\n",
    "    for idx in range(hourly_week_data.shape[0]):\n",
    "        week_data= hourly_week_data.iloc[idx,:].values\n",
    "        week_date= hourly_week_date.iloc[idx,:].values\n",
    "            \n",
    "        # rule1: zero consumption anomalies\n",
    "        if (abs(week_data)<0.001).sum()==seq_len:\n",
    "            zero_consumption_anomalies.append(id_)\n",
    "            continue\n",
    "\n",
    "        # rule2: negative consumption anomalies\n",
    "        week_data_tep = week_data.copy()\n",
    "        week_data_tep[week_data_tep>0.001]=0\n",
    "        if np.nansum(week_data_tep)<0:\n",
    "            negative_consumption_anomalies.append(id_)\n",
    "            continue\n",
    "\n",
    "        # rule3: burty consumption anomalies\n",
    "        mean_usage = []\n",
    "        for i in range(7):\n",
    "            mean_usage.append(np.nanmax(week_data[i*24:i*24+24]))\n",
    "        mean_usage = np.nanmean(mean_usage)\n",
    "\n",
    "        if (week_data> max(5*mean_usage,10)).sum()>0:\n",
    "            burty_consumption_anomalies.append(id_)\n",
    "            continue\n",
    "            \n",
    "        sample_num_after_rules+=1\n",
    "    \n",
    "        \n",
    "        week_data = pd.DataFrame([week_data], columns=week_cols)    \n",
    "        week_date = pd.DataFrame([week_date], columns=week_cols)\n",
    "\n",
    "        hourly_week_data_preprocessed = pd.concat([hourly_week_data_preprocessed, week_data], ignore_index=True)\n",
    "        hourly_week_date_preprocessed = pd.concat([hourly_week_date_preprocessed, week_date], ignore_index=True)\n",
    "        week_num+=1\n",
    "            \n",
    "    if week_num>0:\n",
    "        hourly_week_data_preprocessed.to_csv(path.join(hourly_week_data_afterrules_path, f_name), index=False)\n",
    "        hourly_week_date_preprocessed.to_csv(path.join(hourly_week_date_afterrules_path, f_name), index=False)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
